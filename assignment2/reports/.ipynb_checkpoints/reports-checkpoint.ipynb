{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Assignment 2</center></h1>\n",
    "<h4><center>Pin-Jie Lin</center></h4>\n",
    "<center>pili00001@stud.uni-saarland.de</center>\n",
    "<center>https://github.com/pjlintw/CL20/tree/main/assignment2</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Zipf's law* is one of the most powerful statistical law that the frequency distribution of words in a language obeys a linear pattern when plotting the word frequency and its rank. In this section, we empirically evaluate Zipf's law on 4 corpus and across 3 languages. We receive the similar results when plotting the Zipf's law by two charts with the linear axes and the log axes. We also demonstrate that the law holds in mostly parts of line, namely the line with mostly perfectly slope -1.  We uses the King James Bible, the Jungle Book and the SEtimes Turkish-Bulgarian parallel newspaper text for our experiements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Experiements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we briefly introduce our datasets: the King James Bible, the Jungle Book and the SETimesTurkish-Bulgarian parallel newspaper text. In addition, we descript our implemation detail here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset.** In order to evalute the Zipf's law, we select four corpus to see whether it is true for different domains of text and languages. First, **the King James Bible (KJB)**  is an English translation of the Christian Bible which consists of 31102 sentences. Second, **the Jungle Book** is a collection of stories which has roughly 54887 words counting from segments. For last two corpora, we use parallel news texts, **SETimes Turkish-Bulgarian corpus**, which base on the SETimes.com news portal and consist 206071 parallel sentences in both Turkish and Bulgarian language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experimental settings.** We uses two scripts for building the data and ploting the Zipf's law charts separately. For each corpus, we count its word frequency and store word-frequency pair as kay-value mapping in a dictionary sorting by descending order. We tokenize sentence by whitespace and remain all stop words and punctuation remarks. Because it does not effect the result of zipf's law. In implementation, we collect all corpus on a folder and preprocess them within a loop. We dump the word-frequnecy pairs to a `.json` file for each corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/carbon.png\" width=\"600\"><center>Figure 1. We create a dictionary of word-frequency pairs for each corpus aund dump it to a `.json` by descending order.  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second program `run_zipf.py`, we load the word-frequency pairs from JSON file. In order to compare the results of different corpus, we draw the linear and the log line charts in one image using `plt.subplots` object. It creates a figure and a set of subplots for us. It's convience to us for observing the results when displaying horizontally. We will display the results and disccuss it on next section. For intance, our plotting function was designed to draw both the linear and the log line charts by using the methods `plt.subplots.plot()` and `plt.subplot.loglog()` provided by `matplotlib` library. We demontrate the function on below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/carbon_plot_fn.png\" width=\"600\"><center>Figure 2. The plotting function takes two lists as input for drawing the x-axis, y-axis in linear and log scale. If argument `_plot_img` is given, the figure will be saved in the path.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of the word freqency** In this section, we draw the results of the King James Bible and the Jungle Book and observed that high frequency words occur way more than other words for each corpus. The slope of high frequency words is vertical line comparing to the rest of word frquencies. In addition, it displays a almost perfect line with slope -1 in both figures when applying logarithm to the frequency and its rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"../freq-src-junglebook.jpeg\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "<td> <img src=\"../freq-src-kingjamesbible_tokenized.jpeg\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "</tr></table>\n",
    "<center>Figure 3. The plotting results of the King James Bible and the Jungle Book.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evalute the statistical law on non-English corpus, we plot the results on the news in Bulgarian and Turkish here. Similarly, the highest frequecy words display as a vertical line and are the most majority of each text. And the log-scale line are still a line with pretty close to solpe -1. We can say that the Zipf's law holds on different genre of texts, such as religion, story and news. Meanwhile, it is true for text in different language, such as in Bulgarian and Turkish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"../freq-src-SETIMES.bg-tr.bg.jpeg\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "<td> <img src=\"../freq-src-SETIMES.bg-tr.tr.jpeg\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "</tr></table>\n",
    "<center>Figure 4. The plotting results of the SEtimes news in Bulgarian and in Turkish.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclusion, we demonstrate that 4 corpora in different domains and in different langages have similar results and the Zipf's law makes most erros for lowest frequency words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we train ngram model on arithmetic questions with different context size. \n",
    "Supprisingly, the 5-gram model is able to form correct equation and solve the questions when the model has seen the completed equation during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Problem Definition and Generating Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Definition** Textual generative model has shown promissing results in recent years. In this section, we try to train our ngram model to solve simple 2 digits arithmetic questions for addition and substraction operation. \n",
    "\n",
    "\n",
    "\n",
    "**Generating Training Data** We generative **10000** artifical questions by simply random sampling a number between [0, 99] and substract it by a small number to form an addtion or substraction quation. In our training data,  we add a separate token `[SEP]` betwenn each equation, for example `3 + 2 = 5 [SEP] 13 - 1 = 12`, to provide the equation's boundary. We train 2-gram, 3-gram, 4-gram and 5-gram models to see whether it can generate correct token or solve the equation. We discuss the results in next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/carbon_gen.png\" width=\"600\"><center>Figure 5. The function generates arithmetic questions by sampling a flag for addition and substraction.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 Experment and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mention before, we creates the training data for training different ngram model. Each sequence of tokens is a simple equation for addition or substraction. We concatenate all equations by a separate token `[SEP]` to provide a break signal between each equation. We hope that `[SEP]` could provide useful information to ngram model for generating correct token when given this separate token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2, 3, 4-gram** Due to the context size, these n-gram models can not seen the completed addition or substraction question during training. Therefore, they aren't able to generate correct result to the arithmetic questions. \n",
    "\n",
    "But since the training data has the same pattern, they are able to generate \"right\" token after the context. For example, it always generates a digit after token `=` or after separate token `[SEP]` and don't generate operation token, like `+`, `-`, given another operation symbol. In addition, the trigram and quadrigram models could form the question in correct way althought they can't sample correct digit to the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"./img/carbon-2gram.png\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "<td> <img src=\"./img/carbon-3gram.png\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "<td> <img src=\"./img/carbon-4gram.png\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "</tr></table>\n",
    "<center>Figure 6. The results of 2-gram, 3-gram and 4-gram models generating sequence of arithmetic questions.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-gram** The 5-gram model shows the surprising result to us. It can form 16 correct quations and give the correct number to them. Given a context like `1 + 34 =`, it can produce correct textual number 35 by sampling the distributions of token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"./img/carbon-5gram.png\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "</tr></table>\n",
    "<center>Figure 7. The results of 5-gram model generating sequence of arithmetic questions.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Denpendence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will apply pointwise mutual information on Yelp's reviews provided by [Li et al. (2018)](https://github.com/lijuncen/Sentiment-and-Style-Transfer) and discuss the results fo independence assumption testing on negative reivews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data and experimental setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yelp reviews.** This dataset was released for evaluating the sentiment transfer task in 2018. They consis 266041 postive and 177219 negative reviews on Yelp in the training set, 2000 reviews for each positive and negative for the validation set and 1000 reviews for testing set. We only use the 2000 negative reviews for our experiemnets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experimental setting.** Similar to first section, we tokenize sentence by whitespace and do not remove any stop words and punctuation marks. We set a minimum frequency 10 for filtering words which occur less than the threshold. We compute the pointwise mutual information of two words by the approxiamtion below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "pmi(w_{1}, w_{2}) \\approx log(\\frac{C(w_{1},w_{2})*N}{C(w_{1})*C(w_{2})}) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In implement, we create two dictionaries collecting bigram-frequency and segment-frequency pairs and compute all PMI score for word pairs which occur more than minimum frequency. We demonstrate the function implement in Python bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/carbon_pmi_fn.png\" width=\"600\"><center>Figure X. The function takes `bi2freq`, `seg2freq`, `num_words` as arguments and returing another dictionary object for bigram and PMI Score mapping.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20 highest PMI socres.** It showns two interesting facts in the rank. First, word pairs with higher PMI score are used to judge or express the sentiment of reviewers, like **(zero stars)**, **(worse than)**, **(avoid coming)** and **(poor quality)**. Second, they are reasonably a short phrase that people use a lot in daily life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **20 lowest PMI socres.** The two adjacent words with lower PMI score occur rarely together or sometimes are typo. For example, the word pair **(to and)** is from the sentence: *\\\"i do not steal , do n't have to and never have .\\\"*. The pair isn't a phrase and don't make sense when occurring togehter. And another example is the word pair 19 **(the the)** is a typo in the sentence: *\\\"there is no plug for the the drain.\\\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><th>Highest 20 sscores</th><th>Lowest 20 scores</th></tr>\n",
    "<tr><td>\n",
    "\n",
    "| rank | words | PMI score   |\n",
    "|------|------|------|\n",
    "|1|zero stars | 6.022394923337167 |\n",
    "|2|stay away | 5.91525028588584 |\n",
    "|3|know how | 5.752731356388065 |\n",
    "|4|worse than | 5.723981943102078 |\n",
    "|5|walk away | 5.65742117658374 |\n",
    "|6|an hour | 5.616599182063484 |\n",
    "|7|your money | 5.3850065762627475 |\n",
    "|8|coming back | 5.3472662482799 |\n",
    "|9|avoid coming | 5.3472662482799 |\n",
    "|10|away from | 5.334021021529879 |\n",
    "|11|else where | 5.309114482315524 |\n",
    "|12|make sure | 5.267223540606364 |\n",
    "|13|slow slow | 5.262108439939594 |\n",
    "|14|poor quality | 5.1978888472052995 |\n",
    "|15|as well | 5.074274891238122 |\n",
    "|16|first off | 5.070055475695415 |\n",
    "|17|\\`\\` quality | 5.051285373013424 |\n",
    "|18|point where | 5.051285373013424 |\n",
    "|19|: where | 5.051285373013424 |\n",
    "|20|better than | 5.0055169545578435 |\n",
    "\n",
    "</td><td>\n",
    "\n",
    "| rank | words | PMI score |\n",
    "|------|------|------|\n",
    "|1| is . | -1.8507871228332147 |\n",
    "|2|to and | -1.9527187885548178 |\n",
    "|3|and to | -1.9527187885548178 |\n",
    "|4|it i | -1.9583698954520934 |\n",
    "|5|to , | -1.987715630591914 |\n",
    "|6|and , | -1.987715630591914 |\n",
    "|7|i not | -1.9896923665811346 |\n",
    "|8|the is | -2.052036689512688 |\n",
    "|9|this . | -2.061412455790021 |\n",
    "|10|to was | -2.071414649468561 |\n",
    "|11|so . | -2.072973278191097 |\n",
    "|12|_num_ the | -2.1607408911135777 |\n",
    "|13|but . | -2.2021850096711035 |\n",
    "|14|not . | -2.338867996457971 |\n",
    "|15|do . | -2.419249514908931 |\n",
    "|16|n't . | -2.516036799856269 |\n",
    "|17|for . | -2.6440177619501424 |\n",
    "|18|i the | -3.1215207052577743 |\n",
    "|19|the the | -3.2664807937059197 |\n",
    "|20|was . | -3.625658374032748 |\n",
    "\n",
    "</td></tr> </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclusion, the probability of two random words w1 and w2 has its value when they were observed in the corpus, othwise the probability of pmi(w1, w2) will be zero. However, we use really small corpus that can not capture every grammatical correct or meaningful phrase of english language. For example, the PMI score for a common phrase **(take into)** is 0 since the word pair doesn't appear in the dataset. In general, we can add a small absolute value for C(w1, w2) to avoid the probability of these two words beiing 0. In the future, we would like to modify the approximation by some smoothing techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CL20] *",
   "language": "python",
   "name": "conda-env-CL20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
