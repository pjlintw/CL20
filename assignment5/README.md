# CL20: Assignment 5

Topic modeling: Latent Dirichlet Allocation using Gibbs sampling. The Implementation of LDA model 
automatically discovers topics that documents contain. The model was trained on 2000 movie views with 20 topics and 500 iterations.


## To-Do
1. LDA and Gibbs sampling
2. Most frequent word
3. Report 

## File structure 

```
|--data
|  |-- movices-pp.txt
|  └-- vocab.txt
|
|-- images
|   └-- img-v1.pn
|
|-- report
|   └── report.pdf
|
|-- results
|   └── 2021-01-25_01-00-15
|       |-- main.log
|       |-- param.json
|       |-- out.word
|       |-- zw-iteration100.npz
|       └-- mw-iteration100.npz
|
|-- build_vocab.py
|-- LDA.py
|-- plot_frequency.py
|-- run_analysis.py
|-- run_topk.py
└── README.md
```



## Reports

the report is in `/reports/reports.pdf`.

## Setup and Data preparation

1. python version and dependencies 

We uses python 3.7. Before execute file, please install the dependencies:
`pip install -r requirements.txt`

2. prepare data and evaluation script

The implementation utilise sentence files under the `data` folder. 
Make sure those files (`hansards.f`) are included.

The movie review file `data/movies-pp.txt` that contain one document per line, each words separated by space.

```
python build_vocab.txt
```


### Result Files 

The main script `LDA.py` creates a folder to store log, most k frequent words file, model's hyperparameters and 
learned matrix under the `/results/`. All the files were collected in `results/#RESULT/`.

The result folder was named as one in datetime format `year-month-data_hour-minute-second`. For instance, the result
folder `2021-01-25_01-00-15` stores every files generated by `LDA.py` program. 

* `results/2021-01-25_01-00-15/main.log`: Log file for running LDA file `LDA.py`.
* `results/2021-01-25_01-00-15/params.json`: Parameters for LDA class.
* `results/2021-01-25_01-00-15/out.word`: K most frequent words for each topic with normalized frequency in 2D top-word array per line.  
* `results/2021-01-25_01-00-15/mz-iteration#NUMBER.npz`: Numpy npz file 2D-array, numbe of times document `m` and topic `z` co-occur.
* `results/2021-01-25_01-00-15/zw-iteration#NUMBER.npz`: Numpy npz file, 2D-array, number of times topic `z` and word `w` co-occur.

## Run the LDA with Gibbs sampling

### Basic Usage

Our aligner provides simialr user-interative-command as the baseline aligner
To run our code, you can do: 

```
python LDA.py 
```

### Visualize Top k word



```
python visual.py
```

## Runtime

## Results

To evaluate the result, you should use `score-alignments`. We uses the scripts for all the experiements we have tried.

```
python score-alignments < myIBM-1k
```

The figure shows the top 10 words in the 20 topics generated by LDA over 2000 movie review.

![alt text](./images/img-v1.png)


## Analyse the word distribution over 20 topics 




