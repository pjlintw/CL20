# CL20: Assignment 5

Topic modeling: Latent Dirichlet Allocation using Gibbs sampling. The Implementation of LDA model automatically discovers topics that documents contain. The model was trained on 2000 movie views with 20 topics and 500 iterations.


## To-Do
1. LDA and Gibbs sampling
2. Most frequent word
3. Report 

## File structure 

```
|--data
|  |-- movices-pp.txt
|  └-- vocab.txt
|
|-- images
|   └-- img-v1.pn
|
|-- report
|   └── report.pdf
|
|-- results
|   └── 2021-01-25_01-00-15
|       |-- main.log
|       |-- param.json
|       |-- out.word
|       |-- zw-iteration100.npz
|       └-- mw-iteration100.npz
|
|-- build_vocab.py
|-- LDA.py
|-- plot_frequency.py
|-- run_analysis.py
|-- run_topk.py
└── README.md
```


## Reports

the report is in `/reports/reports.pdf`.

## Setup and Data preparation

1. python version and dependencies 

We uses python 3.7. Before execute file, please install the dependencies:
`pip install -r requirements.txt`

2. prepare data and evaluation script

The implementation utilise movie reviews under the `data` folder. Make sure this file (`movies-pp.txt`) are included.

The movie review file `data/movies-pp.txt` contains one document per line, each word was separated by whitespace.

3. build vocabulary file for running `LDA.py`

Run the maind to build vocabulary for movie reviews.

```
python build_vocab.txt
```

### Result Files 

The main script `LDA.py` creates a folder to store log, most k frequent words file, model's hyperparameters and 
learned matrix under the `/results/`. All the files were collected in `results/#RESULT/`.

The result folder `#RESULT` was named as one in datetime format `year-month-data_hour-minute-second`. For instance, the result
folder `2021-01-25_01-00-15` stores every files generated by `LDA.py` program. 

* `results/2021-01-25_01-00-15/main.log`: Log file for running LDA file `LDA.py`.
* `results/2021-01-25_01-00-15/params.json`: Parameters for LDA class.
* `results/2021-01-25_01-00-15/out.word`: K most frequent words for each topic with normalized frequency in 2D top-word array per line.  
* `results/2021-01-25_01-00-15/mz-iteration#NUMBER.npz`: Numpy npz file 2D-array, numbe of times document `m` and topic `z` co-occur.
* `results/2021-01-25_01-00-15/zw-iteration#NUMBER.npz`: Numpy npz file, 2D-array, number of times topic `z` and word `w` co-occur.

## Runtime

We speed up the calculation by using numpy to create the counting matrices and vectors. 

To sample a topic `z` from multinomial distribution, we count the `document-topic` and `topic-word` co-occurences in the 2D arrays with shapes (`number of documents`, `number of topics`) and (`number of topics`, `vocabulary size`). The co-occurence matrices were normalized by the total number of topics for each document and number of words for each topics vectors separately. In gibbis sampling, the probability of topic `z` for specific word `w` at position `i` in a document is proportional to the multiplication of the two normalized matrices. 

We ran the `LDA.yp` on 2000 movie reviews with the hyperparameters `alpha=0.02`, `beta=0.1`, 500 iterations and 20 topics in **2 hours 47 minutes**. We save the unnormalized topic-word matrix as `npz` files every `100` iterations (`save_per_iteration=100`). The runningtime records can be found in lgo file `results/2021-01-25_01-00-15/main.log`. 

We also ran the program with same hyperparameters but using `alpha=50` in **2 hours 55 minutes**. The log file exists in the path `results/2021-01-25_11-14-11/main.log`. 

## Run the LDA with Gibbs sampling

### Basic Usage

Before running the main script `LDA.py`, make sure that `data/vocab.txt` exists in the path.

In the `main` function, we set hyperparameters with `alpha=0.02`, `beta=0.1`, `n_iteration=500`, `n_topic=20`, `top_k=10` and `save_per_iteration=100` as default for training LDA on movie reviews. The program trains LDA model with `n_topic` latent topic variables and will save `document-topic` as `topic-word` co-occurence matrices every 100 iterations. Most `top_k` frequent words will be saved in a text file `out.word` with the 
correspond value in `topic-word` matrix per line. 

You can run it in the default setting. All the relevant files will be stored in the result folder.

```
python LDA.py 
```

### Visualize Top k word

After running the main script, k most frequent words for each topic and the corresponding values will be exported as text file `out.word` in result folder. 

To visualize the most `top_k` frequent words for each topic, we plot `n_topic` bar charts ranked by its frequency. The figure will be saved in `images` folder with the name of `result folder`. You can run the command.

```
python run_topk.py
```

## Results

We run the LDA model in two hyperparameter settings and analysis the learned `topic-word` matrix. We found that the LDA model trained with default hyperparameter assigns movie relevant words to latent topics in the firs result. But as we modity `alpha`, the most frequent words in the topic seems don't have specific meaning. Finnaly, we can compute word similarity using the learned matrix. 

* Optimial LDA
* Alpla-20 LDA model
* Analysis word distribution over topics

Note: the bold text is the words selected in the latent topics.

### Optimal LDA  

The first figure shows the top 10 words in the 20 topics generated by LDA with `alpha=0.02` and `beta=0.1` over 2000 movie review in 500 iterations.

![alt text](./images/2021-01-25_01-00-15.png)

We observe 11 topics in the result trained on default setting: 

* **Movie characters (topic 1)**
* **Space science (topic 4)**
* **Boogie nights (topic 5)**: disco, Paul Thomas Anderson  
* **Frequent words (topic 11, 13)**: most frequent words in the document file. 
* **The wedding spinger (topic 12)** is a group of elements for the the american romantic film.
* **Martials art (topic 14)**:  
* **Disney animation (topic 15)**: comedy, julia,the animated movie Titanic, Mulan
* **Horroric film (topic 18)**:    
* **Shakespeare (topic 19)**: 
* **Crime films directed by Tarantino (topic 20)**: pulp fiction, jackie nrown  
* **Pulp fiction (topic 20)**: Quentin Tarantino. 

**Movie characters (topic 1)** are mostly charactor, like **batman**, **godzilla**, **arnold schwarzenegger**. And most topics are related to movie or genre. For intance, the frequent words in **Space science (topic 4)** are related to *star trek*, such as **alien**, **aliens**, **planet**, **wars**, **space**, **mars**, **planets**.

Similar topics like **Boogie nights (topic 5)**, **The wedding singer (topic 12)**, **Martials art (topic 14)**, **Disney animation (topic 15)** and **Pulp fiction (topic 20)** are a topic for specific movie or animation:  **Boogie nights (topic 5)** is period drama film directed by Paul Thomas **Anderson** and the story is in the late 1970s **disco** era. **The wedding singer (topic 12)** is **romantic comedy*** film produced by **Robert**.
im

**Martial ars**. The martial artist Jackie Chan were in the topic. 



**Frequent words (topic 11, 13)** are a special case in our result. The frequent words in theses topics have higher frequencise than the words in the other topics. The most frequent words in other topics (`2021-01-25_01-00-15/out.word`) have the ammount under one thousand on averag. But most frequent words were highly sampled and assigned to tipic 11 and topic 13.

The two topics learn collecting most frequent words in the movie articles. We count the word frequency by running the script `plot_frequency.py`. It writes vocabulary file with frequency `data/frequent-wrod.txt` and plot the top 300 frequent words bar chart saved in data folder `data/wordFrequency-3000.png`.

![alt text](./images/wordFrequency-300.png)
*We plot top 30 frequent words in movie reviews out of 46517 words. A few words have the dominant frequency than other word.*

The most frequent words in topic 11 and 13 are mostly from the high frequent words in the movie reviews. We list most 20 frequent words in the file bellow. The most frequent 6 words in the topic 13 are the most 6 frequent words in the movies reviews. Topic 11 does not have same frequency order as topic 13. But they covers 14 frequent words out of most 20 words in the file.

```text
1. **film** 9443
2. **movie** 5671
3. **one** 5580
4. **like** 3545
5. **even** 2556
6. **good** 2316
7. **time** 2282
8. **would** 2264
9. **story** 2145
10. **much** 2024
11. **character** 1996
12. also 1965
13. **get** 1925
14. **characters** 1858
15. **two** 1827
16. first 1769
17. see 1731
18. way 1669
19. well 1655
20. could 1609
```

The sampling frequency in the default setting has huge unbalanced phenomena between topic 11, 13 and the other. We are curious about using other hypermeter to observe the smapling frequency and latent topic modeling. Thus, we tried to trained the LDA model by modifying `alpha` as bellow. 

### Alpla-20 LDA model

The second figure shows the model ran in the same setting but using `alpha=50`. All the other hyperparameters are same as previous one.

![alt text](./images/2021-01-25_11-14-11.png)


The high frequent word in the movie reviews, such as `film`, `movie`, `good`, `like` and `one`, occur in our result. But they are not in a topic group. 


And we can not even find any group that has inner similarity between their frequent words.



### Analyse the word distribution over 20 topics 

We utilise the learned matrix to by computing words similarity.

