# CL20: Assignment 5

Topic modeling: Latent Dirichlet Allocation using Gibbs sampling. The Implementation of LDA model automatically discovers topics that documents contain. The model was trained on 2000 movie views with 20 topics and 500 iterations.


## To-Do
1. LDA and Gibbs sampling
2. Most frequent word
3. Report 

## File structure 

```
|--data
|  |-- movices-pp.txt
|  └-- vocab.txt
|
|-- images
|   └-- img-v1.pn
|
|-- report
|   └── report.pdf
|
|-- results
|   └── 2021-01-25_01-00-15
|       |-- main.log
|       |-- param.json
|       |-- out.word
|       |-- zw-iteration100.npz
|       └-- mw-iteration100.npz
|
|-- build_vocab.py
|-- LDA.py
|-- plot_frequency.py
|-- run_analysis.py
|-- run_topk.py
└── README.md
```


## Reports

the report is in `/reports/reports.pdf`.

## Setup and Data preparation

1. python version and dependencies 

We uses python 3.7. Before execute file, please install the dependencies:
`pip install -r requirements.txt`

2. prepare data and evaluation script

The implementation utilise movie reviews under the `data` folder. Make sure this file (`movies-pp.txt`) are included.

The movie review file `data/movies-pp.txt` contains one document per line, each word was separated by whitespace.

3. build vocabulary file for running `LDA.py`

Run the maind to build vocabulary for movie reviews.

```
python build_vocab.txt
```

### Result Files 

The main script `LDA.py` creates a folder to store log, most k frequent words file, model's hyperparameters and 
learned matrix under the `/results/`. All the files were collected in `results/#RESULT/`.

The result folder `#RESULT` was named as one in datetime format `year-month-data_hour-minute-second`. For instance, the result
folder `2021-01-25_01-00-15` stores every files generated by `LDA.py` program. 

* `results/2021-01-25_01-00-15/main.log`: Log file for running LDA file `LDA.py`.
* `results/2021-01-25_01-00-15/params.json`: Parameters for LDA class.
* `results/2021-01-25_01-00-15/out.word`: K most frequent words for each topic with normalized frequency in 2D top-word array per line.  
* `results/2021-01-25_01-00-15/mz-iteration#NUMBER.npz`: Numpy npz file 2D-array, numbe of times document `m` and topic `z` co-occur.
* `results/2021-01-25_01-00-15/zw-iteration#NUMBER.npz`: Numpy npz file, 2D-array, number of times topic `z` and word `w` co-occur.

## Runtime

We speed up the calculation by using numpy to create the counting matrices and vectors. 

To sample a topic `z` from multinomial distribution, we count the `document-topic` and `topic-word` co-occurences in the 2D arrays with shapes (`number of documents`, `number of topics`) and (`number of topics`, `vocabulary size`). The co-occurence matrices were normalized by the total number of topics for each document and number of words for each topics vectors separately. In gibbis sampling, the probability of topic `z` for specific word `w` at position `i` in a document is proportional to the multiplication of the two normalized matrices. 

We ran the `LDA.yp` on 2000 movie reviews with the hyperparameters `alpha=0.02`, `beta=0.1`, 500 iterations and 20 topics in **2 hours 47 minutes**. We save the unnormalized topic-word matrix as `npz` files every `100` iterations (`save_per_iteration=100`). The runningtime records can be found in lgo file `results/2021-01-25_01-00-15/main.log`. 

We also ran the program with same hyperparameters but using `alpha=50` in **2 hours 55 minutes**. The log file exists in the path `results/2021-01-25_11-14-11/main.log`. 

## Run the LDA with Gibbs sampling

### Basic Usage

Before running the main script `LDA.py`, make sure that `data/vocab.txt` exists in the path.

In the `main` function, we set hyperparameters with `alpha=0.02`, `beta=0.1`, `n_iteration=500`, `n_topic=20`, `top_k=10` and `save_per_iteration=100` as default for training LDA on movie reviews. The program trains LDA model with `n_topic` latent topic variables and will save `document-topic` as `topic-word` co-occurence matrices every 100 iterations. Most `top_k` frequent words will be saved in a text file `out.word` with the 
correspond value in `topic-word` matrix per line. 

You can run it in the default setting. All the relevant files will be stored in the result folder.

```
python LDA.py 
```

### Visualize Top k word

After running the main script, k most frequent words for each topic and the corresponding values will be exported as text file `out.word` in result folder. 

To visualize the most `top_k` frequent words for each topic, we plot `n_topic` bar charts ranked by its frequency. The figure will be saved in `images` folder with the name of `result folder`. You can run the command.

```
python run_topk.py
```

## Results

* baseline model with 
* alpla-20 LDA model
* Analysis word distribution


We run the LDA model in two hyperparameter settings and discuss the k frequent words for 20 topics as follows:

The first figure shows the top 10 words in the 20 topics generated by LDA with `0.02` and `beta=0.1` over 2000 movie review in 500 iterations.

![alt text](./images/2021-01-25_01-00-15.png)

We observe a few topics for the result. 

**Movie characters ()**: 
**Space science ()**: 
**Boogie nights (topic 5)** disco, Paul Thomas Anderson
** Martials art (topic 12)**:
**(romantic comediy**: comedy, julia
**Crime file directed by Tarantino (topic 20)**: pulp fiction, jackie nrown
**tfrequent words (topic 11, 13)**


Note: the underlying text is the words selected in the topics.

The second figure shows the model ran in the same setting but using `alpha=50`. 

**martial ars**

**Frequent words (topic 11, 13)**. In th



The martial artist Jackie Chan were in the topic
<span style="color:blue">some *blue* text</span>.
```text
<span style="color:blue">film</span> 9443
movie 5671
one 5580
like 3545
even 2556
good 2316
time 2282
would 2264
story 2145
much 2024
character 1996
also 1965
get 1925
characters 1858
two 1827
first 1769
see 1731
way 1669
well 1655
could 1609
```


**romatic comedy (topic 12)** is a group of elements for the the american romantic film The Wedding Spinger. The

**Pulp fiction (topic 20)**. Quentin Tarantino. 

![alt text](./images/2021-01-25_11-14-11.png)



## Analyse the word distribution over 20 topics 

## Analyse word distribution matrix



